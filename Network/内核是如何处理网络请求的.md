# 内核是如何处理网络请求的



## 概述



### 系统和网卡初始化阶段

1. 每个CPU绑定一个**Ksoftirqd线程**用于处理接受消息的软中断流程；

2. 初始化网络系统，在每个CPU中初始化**softnet_data**的数据结构，其中的**poll_list**用于告诉CPU哪些设备里有待读取的网络数据帧；

3. 注册各种类型**软中断的回调函数**；

4. 初始化**协议栈的回调函数**；

5. 初始化网卡，网卡会实现与内核定义好的接口函数和注册变量提供内核调控网卡的能力。

   

### 启动阶段:

​	网卡将初始化多个接收队列和发送队列，并为队列注册相应的硬中断



### 接受数据阶段：

1. 网络数据帧先会将数据发送到网卡，网卡通过DMA机制将网卡数据传输到内存的RingBuffer中，之后RingBuffer会发送一个硬中断给对应的CPU，CPU接受到硬中断信号后，会将网卡中poll_list 数据追加到cpu的寄存器的softnet_data中，并设置软中断标志。
2. 当CPU对应的Ksoftirqd线程监听到软中断标志变化，则会异步的进行软中断处理，首先会将硬中断机制屏蔽，并从CPU的寄存器中遍历poll_list，通过网卡注册到内核NAPI的Poll函数 ，将设备的中数据帧从RingBuffer读取出来，并为其加上协议信息，交由协议层处理





## 流程图



![image-20220628043549962](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206280435003.png)



## 初始化阶段



### softirq_vec

软中断注册函数集合：每个中断的处理函数都被注册在该数组**softirq_vec**中

![image-20220627224608697](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206272246716.png)

### smp_register_percpu_thread 

将某个线程与CPU进行绑定

![image-20220627220916255](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206272209283.png)



#### 1. Ksortirqd 线程初始化

**系统初始化阶段**，会为**每一个CPU绑定softirqd线程**，并进行自循环，监听软中断事件。



- CPU绑定ksoftirqd线程，内部包含检查软中断标志函数（should_run），与软中断执行函数（run_ksoftirqd）

  ![image-20220627221520804](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206272215828.png)

- 检查软中断标志函数（should_run），与软中断执行函数（run_ksoftirqd）

  ![image-20220627221825535](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206272218558.png)

- 注册硬中断处理事件

  ![image-20220627223146633](C:/Users/92175/AppData/Roaming/Typora/typora-user-images/image-20220627223146633.png)

#### 2. Poll 收包函数与网络发送和接收包软中断注册

Linux 在subsys_initcall 初始化各个子系统，网络子系统的初始化会调用net_dev_init 函数为每个CPU初始化softnet_data, 其中的**poll_list**用于在网卡接收数据时，通过硬中断，将网卡设备可以读取的数据添加到poll_list中，再通过网卡注册的Poll函数进行读取，另外，会注册网络发送和接收包软中断处理函数net_rx_action 与 net_tx_action

- **为每个CPU初始化softnet_data**

  ![image-20220627223821038](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206272238061.png)

  - softnet_data(部分)

  ![image-20220627224155398](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206272241419.png)

  

- **注册软中断处理函数**， net_tx_action 消息发送处理函数， net_tx_action 消息接收处理函数

  ![image-20220627224404178](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206272244197.png)



#### 3. 协议栈处理注册

和subsys_call 类似 fs_initcall也是初始化模块的入口，通过调用**fs_initcal(inet_init)【重要】**初始化各类协议栈的处理函数，其中**TCP和UDP**协议处理函数被注册在 **inet_protos** 数组中，**IP**协议处理函数被注册在 **ptype_base** 哈希表中

- TCP -- tcp_rcv

  ![image-20220627230813595](C:/Users/92175/AppData/Roaming/Typora/typora-user-images/image-20220627230813595.png)

- UDP -- udp_rcv

  ![image-20220627230623529](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206272306548.png)

- ICMP -- icmp_rcv

  ![image-20220627230802620](C:/Users/92175/AppData/Roaming/Typora/typora-user-images/image-20220627230802620.png)

  

  

- IP -- ip_rcv

  ![image-20220627230442660](C:/Users/92175/AppData/Roaming/Typora/typora-user-images/image-20220627230442660.png)

  

  inet_add_protocol 注册协议函数

  ![image-20220627225909016](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206272259035.png)

  ![image-20220627230217884](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206272302902.png)

  

  dev_add_pack 注册协议函数

  ![image-20220627230021216](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206272300233.png)

![image-20220627230325147](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206272303173.png)





#### 4. 网卡驱动初始化

当驱动程序被加载时，会调用驱动程序初始化网卡功能：

1. DMA初始化
2. 注册ethtool实现函数（这里内核可以通过接口的方法去获取网卡的一些信息，我们的ethtool命令也是在这一阶段实现的）
3. 注册网卡启动相关函数【net_device_ops】
4. 注册NAPI机制的Poll函数，用于软中断收包时调用



## 启动阶段

#### 1. 网卡启动

- 初始化接受队列和发送队列【图为接收队列】

  - SKB是动态生成的用来存储网络数据帧的数据结构
  - rx_buffer与rx_desc 是内核和网卡的环形队列用来指向SKB里的数据

  ![image-20220628013823698](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206280138129.png)

- 网卡在内存中分配多个队列，并为每个队列注册硬中断函数

  - ```c
    	for (i = 0; i < num_q_vectors; i++) {
    		struct igb_q_vector *q_vector = adapter->q_vector[i];
    
    		vector++;
    
    		q_vector->itr_register = adapter->io_addr + E1000_EITR(vector);
    
    		if (q_vector->rx.ring && q_vector->tx.ring)
    			sprintf(q_vector->name, "%s-TxRx-%u", netdev->name,
    				q_vector->rx.ring->queue_index);
    		else if (q_vector->tx.ring)
    			sprintf(q_vector->name, "%s-tx-%u", netdev->name,
    				q_vector->tx.ring->queue_index);
    		else if (q_vector->rx.ring)
    			sprintf(q_vector->name, "%s-rx-%u", netdev->name,
    				q_vector->rx.ring->queue_index);
    		else
    			sprintf(q_vector->name, "%s-unused", netdev->name);
    
    		err = request_irq(adapter->msix_entries[vector].vector,
    				  igb_msix_ring, 0, q_vector->name,
    				  q_vector);
    		if (err)
    			goto err_free;
    	}
    
    ```

  - 网卡可以开启多个队列，具体限制由网卡决定，用户可以动态的调节队列长度和数量
    ```shell
    查看某个网卡的队列信息
    ethtool -l eth0
    
    修改网卡的队列数量
    ethtool -L eth0 combined 32
    
    查看网卡的某个中断号亲和的cpu
    cat /proc/irq/中断号/smp_affinity #8 代表 1000 表示第4个cpu核[可以通过修改该文件处理中断和cpu的亲和度]
    
    ethtool -G eth1 rx xxx tx xxx // 修改队列的大小
    
    top 可以看到硬中断和软中断的在cpu上的负载1
    
    ifconfig | ethtool -S eth0 可以查看1队列中是否有出现丢包
    ```

    
    
  - 每个队列有独有的硬中断，可以让收到的包给不同的CPU处理[第一列为中断号]

![image-20220628015158388](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206280151412.png)



## 收包阶段

![image-20220628015904352](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206280159384.png)


#### 1. 硬中断处理

- 将网卡中的poll_list 数据追加到内核中的softnet_data 的poll_list 中，从而告知CPU 哪些设备下面有数据需要进行收包读取

  <img src="https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206280208518.png" alt="image-20220628020754897"  />

- 紧接着触发一次软中断, 只涉及一个变量的修改

  ![image-20220628021106393](C:/Users/92175/Documents/%E4%B8%BB%E6%9C%BA%E5%A4%87%E4%BB%BD%E7%A9%BA%E9%97%B4/202206280211413.png)

- 硬中断很轻量，把CPU资源腾出来给其他线程启用，其他重量级的操作放在ksoftirqd线程中异步执行



#### 2. 软中断处理

之前我们提到软中断处理函数会一直循环监听是否有发送软中断事件，在CPU处理完硬中断后，会更改标志位触发软中断处理

- 对于接受网络消息，触发循环调用net_rx_action方法处理

  ![image-20220628023106494](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206280231520.png)

- 在软中断处理函数过程中，会通过local_irq_disable屏蔽CPU硬中断信号，避免在处理过程中poll_list更新【细节】

- 在软中断处理过程中有time_limit 和 budget 对处理过程进行时间上和空间上的限制，避免长时间的运行软中断处理【细节】

  ![image-20220628021628709](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206280227370.png)

- 对softnet_data中的poll_list进行遍历，然后执行NAPI中的Poll函数



![image-20220628022655386](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206280226417.png)

- 对接受队列上的SKB数据进行组装，最终转为一个数据包

  - 通过build_skb | construct_skb 取出ring Buffer上的数据帧
  - 箭头1处理当SKB从接收队列取出时，会申请新的SKB在put进去，给新包使用
  - 通过箭头2和箭头3函数保证能从SKB中取出完整的数据帧
  - 箭头4对取出的数据帧做校验，并设置SKBd1timestamp、VLANid、protocol等字段。
  - 箭头5将小包组合成大包传送给协议层处理。
  
  ![image-20220628023934255](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/202206280239295.png)

#### 3. 网络协议处理

以上步骤已经从数据链层将数据帧打包成数据包发送给协议层了，现在系统会根据包的协议信息，执行对应的协议处理函数

- 从数据包中解析对应的protocol，并执行注册在**ptype_base**中的相应函数，在此之前 ptype_all 类似于一种AOP处理，其中包含了tcpdump虚拟协议，负责采集数据包信息，我们的tcpdump读取的数据就是从这而来。

- 随后取出skb中的protocol信息，在ptype_base中取出对应的IP处理函数执行

  ```c
  static int __netif_receive_skb_core(struct sk_buff **pskb, bool pfmemalloc,
  				    struct packet_type **ppt_prev)
  {
  	
  	。。。。。。。。
  	
  	list_for_each_entry_rcu(ptype, &ptype_all, list) {
  		if (pt_prev)
  			ret = deliver_skb(skb, pt_prev, orig_dev);
  		pt_prev = ptype;
  	}
  
  	list_for_each_entry_rcu(ptype, &skb->dev->ptype_all, list) {
  		if (pt_prev)
                  ret = deliver_skb(skb, pt_prev, orig_dev);
  		pt_prev = ptype;
  	}
     。。。。。。。。
  	type = skb->protocol;
  
  	/* deliver only exact match when indicated */
  	if (likely(!deliver_exact)) {
  		deliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,
  				       &ptype_base[ntohs(type) &
  						   PTYPE_HASH_MASK]);
  	}
  
  	deliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,
  			       &orig_dev->ptype_specific);
  
  	if (unlikely(skb->dev != orig_dev)) {
  		deliver_ptype_list_skb(skb, &pt_prev, orig_dev, type,
  				       &skb->dev->ptype_specific);
  	}
  }
  
  static inline int deliver_skb(struct sk_buff *skb,
  			      struct packet_type *pt_prev,
  			      struct net_device *orig_dev)
  {
  	if (unlikely(skb_orphan_frags_rx(skb, GFP_ATOMIC)))
  		return -ENOMEM;
  	refcount_inc(&skb->users);
  	return pt_prev->func(skb, skb->dev, pt_prev, orig_dev);
  }
  ```

- IP层处理，对应ip_rcv函数, NF_HOOK 函数主要处理各种协议的过滤点，在这里，过滤完成后会调用ip_rcv_finish，进行源地址和目的地址校验，最终根据udp和tcp 协议分发给传输层协议处理

  ```C
  int ip_rcv(struct sk_buff *skb, struct net_device *dev, struct packet_type *pt,
  	   struct net_device *orig_dev)
  {
  	struct net *net = dev_net(dev);
  
  	skb = ip_rcv_core(skb, net);
  	if (skb == NULL)
  		return NET_RX_DROP;
  
  	return NF_HOOK(NFPROTO_IPV4, NF_INET_PRE_ROUTING,
  		       net, NULL, skb, dev, NULL,
  		       ip_rcv_finish);
  }
  
  static int ip_local_deliver_finish(struct net *net, struct sock *sk, struct sk_buff *skb)
  {
  	skb_clear_delivery_time(skb);
  	__skb_pull(skb, skb_network_header_len(skb));
  
  	rcu_read_lock();
  	ip_protocol_deliver_rcu(net, skb, ip_hdr(skb)->protocol);
  	rcu_read_unlock();
  
  	return 0;
  }
  
  ```

  







## 延伸



### 如何处理某个单核CPU负载过高问题

由于我们的硬中断和软中断机制在是在同一个CPU上运行，在某种情况下，有可能某个队列上的消息比较多，导致高于其他CPU的负载，甚至导致丢包，为了处理这种情况，我们可以采取拓展原先队列数量，并修改队列中断号与CPU的亲和度来减缓请求集中处理在某个CPU上的问题。



