# 内核与用户进程的协作

概述： 书接上回，我们讲到数据通过网卡和内核的密切配合，最终被提交到了协议栈函数进行处理，这一章我们讲解如何将数据传达给我们的用户





## 阻塞式IO

当我们的进程上的线程上在收集消息时，因为要等待消息传达，导致了线程进入了等待，让出了CPU，此时该线程进入了等待阶段，除非数据加载好了，否则该线程都不会被唤醒。

![image-20220715105146354](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/img/202207151051425.png)

### 内核与用户的阻塞协作分析

```c
int main (){
	int sock = socket(AF_INET, SOCK_STREAM, 0);
    connect(sock, ...);
    recv(sock, ...);
}
```



1. 用户创建Socket用来管理网络发来的信息
2. 当Socket创建完成后，内核会创建一条接受队列，用来接受从网络发送过来的消息。
3. 用户线程通过Socket API 读取网络的信息，如果此时网络的数据还没有准备好，则用户线程会进入阻塞状态，让出CPU，并将当前线程信息挂靠在Socket上并插入等待队列中。
4. 当网络数据准备好后，会通过我们之前的Ksoftirqd线程处理软中断过程，并唤醒等待队列中的某一个进程。



### 源码解析

```c
```



### 问题

显而易见，当一个客户端的程序去以一个阻塞IO的方式进行网络请求，在返回数据前，啥事都干不了，但这种现象却普遍存在（比如访问一个Mysql数据库，你需要数据返回后在进行业务操作，而不是数据没返回前就进行业务操作）; 但当服务端在处理数据的时候，如果只有一个线程，那么就会导致服务在运行时串行的解决到来的数据，所以服务不得不开启多个线程去处理多个请求，最终可能会导致线程过多内存占用过大服务崩溃；另外一方面，还有不同线程之间在CPU的上下文切换也会造成不小的CPU代价。



## IO多路复用

可以看到，阻塞IO是一种各管各的IO管理方式，每个线程管理自己的阻塞事件，直到他被处理。而IO多路复用，则是请了一位代理人，将Socket委托给他，让他出具一份就绪列表，形成一个代理人管理多个请求，并将数据提交给任意一位委托人，达到多路Socket返回的数据通过代理人复用给各位委托人。

![image-20220715111636865](https://typroa-pic-sh-1258186845.cos.ap-shanghai.myqcloud.com/img/202207151116967.png)

### Event Poll

```c
int main (){
	listen(lfd, ...);
    cfd1 := accept(...);
    cfd2 := accept(...);
    efd = epoll_create(...);
    epoll_ctl(efd, EPOLL_CTL_ADD, cfd1, ...);
    epoll_ctl(efd, EPOLL_CTL_ADD, cfd2, ...);
    epoll_wait(...)
}
```



1. 用户进程首先创建并初始化内核的Epoll对象，此时，内核会为它关联到当前进程已打开的文件列表中。
2. 用户进程将需要监听的Socket端口的文件描述符传递给Epoll对象，此时，内核会创建一个Epoll红黑数管理的树节点，并将该Socket的等待队列插入一个等待队列项，包含唤醒函数和Epoll 红黑树上管理的树节点。
3. 用户进程调用Epoll接收数据的函数，查看Epoll上的就绪队列上是否有就绪的Socket，如果没有，则用户线程会进入阻塞状态，让出CPU，并将当前线程信息挂靠在Socket上并插入等待队列中。
4. 当Socket数据到来时，检查等待队列上是否有数据，如有有，通过调用其唤醒函数，找到Epoll中红黑树管理的树节点，并将设置到Epoll的就绪队列中，并查看Epoll上是否有等待的数据，如果有，唤醒Epoll上阻塞的线程





### 源码分析

```C
```





### 优势

1. 假设有多个请求同时并发，会有多个套接字进行数据传输
   - 如果是阻塞IO，系统线程数和请求数成正比，频繁的用户态和内核态切换（等待与唤醒）和线程调度会浪费许多无用的CPU资源，巨量的线程还有可能耗尽计算机资源
   - 如果是IO多路复用，需要一个Epoll的IO线程管理多个Socket，用户的线程是可以根据CPU支持的线程数进行调配，最差情况下只有有限个线程会出现阻塞，等待数据的到来，但是在一般情况下，Sokcet数据都是不断的通过Socket->Epoll 就绪队列中，不会对用户的线程进行阻塞，减少了线程间的上下文切换
   - 总结：Epoll模型通过代理，减少了用户线程之间的上下文切换和阻塞与唤醒带来的开销，从而让网络的IO达到高效。







## 延伸



### Redis 的网络IO

由于Redis的数据处理都是在内存上的数据读写，早期的Redis将内存数据的处理和网络IO做成单线程，极大的减少了多线程间数据处理协作带来的性能影响。如何减少网络IO带来的性能消耗是决定Redis高性能的关键因素，而Redis就是采用了和Epoll相似的解决思路，通过单个线程对网络IO进行读写，之后和内存进行读写后返回，但随着CPU能力的不断提升，单个线程进行网络IO，不能发挥出多核CPU的优势，于是引入了Reactor模型和Proactor模型去处理网络的IO。





## 参考文献

1. 《深入理解Linux网络》 · 张彦飞
