# ETCD



## 概述

在K8s主宰容器编排的时代，ETCD作为分布式存储的解决方案，在服务发现，分布式锁，配置存储，分布式协调上都有出色的表现。早期K8s的0.4版本正是使用了初期Etcd的0.2版本，当使用K8s的声明式api部署，会**通过ETCD的Watch机制**实时监听资源变化事件，检查容器状态是否和期望状态一致。在2015年1月CoreOS发布了ETCD的第一个稳定版本2.0，支持Quorum Read提供线性的一致性读能力，同年7月基于ETCD2.0的K8s第一个生产环境可用版本1.01正式发布。V2的ETCD依然有许多弊端：内存开销过大，网络沟通的性能瓶颈，Watch机制的可靠性问题。2016年6月V3诞生，随后k8s 1.6发布，默认启用etcd3.0，助力K8s支撑5000节点集群



## 核心技术点

- 数据复制方式
- 共识算法
- 分片算法
- 数据模型
- 存储引擎
- 事务实现



## 基础

### ETCD基础架构

- 多副本提高系统可用性带来的数据一致性和实时性问题，引入Raft共识算法解决多份本数据一致性问题
- 通过CAS解决数据原子更新问题，并从Leader上读取数据，解决读不一致问题
- 提供Get、Set、Del、Watch等与Zookeeper类似的功能
- etcd是典型的读多写少存储



### Raft算法



### 鉴权模块



### 租约模块



### MVCC/Watch模块



## 实践





## 问题

### 1. ETCD Watch 机制能保证事件不丢失吗

### 2. 哪些因素会导致你的集群Leader切换

### 3. 为什么基于Raft实现的ETCD会出现数据不一致

### 4. 为什么你要删除大量数据，但是DB的大小没有减少，而且社区的推荐大小不超过8G

### 5. 为什么集群的各节点磁盘的IO时延很低，写请求也会超时？

### 6. 为什么只存储1个几百KB的数据，ETCD进程却可能消耗数G内存？

### 7. 为什么通过标签查询同一个NameSpace下多个Pod、CRD资源时，ETCD和APIServer 会扛不住？

### 8.与Zookeeper 的差别

1. 通过简单内存树记录内存中的KV信息，与使用Concurrent HashMap的Zookeeper不同

### 9. 与Redis的差别

- 数据复制上，Redis使用的是主备复制，可能出现数据丢失的现象，为了保证读写一致性，etcd的读写性能会比Redis差的多
- 数据分片上，Redis有各种集群版的解决方案，可以承载大量用户数据，而etcd的定位是低容量的关键元数据存储



